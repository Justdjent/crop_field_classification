{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In google colab, \n",
    "# For master version of catalyst, uncomment:\n",
    "# (master version should be fully compatible with this notebook)\n",
    "# ! pip install git+git://github.com/catalyst-team/catalyst.git\n",
    "\n",
    "# For last release version of catalyst, uncomment:\n",
    "# ! pip install catalyst\n",
    "\n",
    "# For specific commit version of catalyst, uncomment:\n",
    "# ! pip install git+http://github.com/catalyst-team/catalyst.git@{commit_hash}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have Unet, all CV is segmentation now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- train Unet on isbi dataset\n",
    "- visualize the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-14 21:47:44--  https://www.dropbox.com/s/0rvuae4mj6jn922/isbi.tar.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.1, 2620:100:6026:1::a27d:4601\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/0rvuae4mj6jn922/isbi.tar.gz [following]\n",
      "--2019-06-14 21:47:44--  https://www.dropbox.com/s/raw/0rvuae4mj6jn922/isbi.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc0226b5ea0e46c5db64e70c39ff.dl.dropboxusercontent.com/cd/0/inline/Ai10dwDkpz2sqn7NXElA5DDsDLdBuITZrU7n4zAJoOaUwfkXJgymOVMXpZgRAJN5lJfA_3tJuaEo42qhx4_im76YO1bh7zC56YQ_LhTGE91QEg/file# [following]\n",
      "--2019-06-14 21:47:45--  https://uc0226b5ea0e46c5db64e70c39ff.dl.dropboxusercontent.com/cd/0/inline/Ai10dwDkpz2sqn7NXElA5DDsDLdBuITZrU7n4zAJoOaUwfkXJgymOVMXpZgRAJN5lJfA_3tJuaEo42qhx4_im76YO1bh7zC56YQ_LhTGE91QEg/file\n",
      "Resolving uc0226b5ea0e46c5db64e70c39ff.dl.dropboxusercontent.com (uc0226b5ea0e46c5db64e70c39ff.dl.dropboxusercontent.com)... 162.125.70.6, 2620:100:6026:6::a27d:4606\n",
      "Connecting to uc0226b5ea0e46c5db64e70c39ff.dl.dropboxusercontent.com (uc0226b5ea0e46c5db64e70c39ff.dl.dropboxusercontent.com)|162.125.70.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: /cd/0/inline2/Ai0-Xq4jsjPpzY_inIDMOjq404Seoaz_NEYXkhM9erHjc_ALrnu7zlKEsZNvJClwC7_OeDrz_h9bvh_YCY08MUDT4cx9QtcrErCM1xlfWCVblfMEY9yWvA2IVHpcjXEO0yY__86fSbv2Krw15Du2nlnMFyNH-BJVHVqhWsCxGPBzfQNJsAhD7BBCZyf37g_1UPWrJDOp0fH36gS9nRPLQZ1yN8sjsa9f79BIeWX-_LefcyFQi_By5tSRN2bIOlSqppjK_NPXuGpvSfjH0bjNcr5iWt4NYn6c1JWJMXp2hQxtH0pqYAAa8dYQsNlUSocjnMTNJWtRcafKyyVz96X4kt-m/file [following]\n",
      "--2019-06-14 21:47:47--  https://uc0226b5ea0e46c5db64e70c39ff.dl.dropboxusercontent.com/cd/0/inline2/Ai0-Xq4jsjPpzY_inIDMOjq404Seoaz_NEYXkhM9erHjc_ALrnu7zlKEsZNvJClwC7_OeDrz_h9bvh_YCY08MUDT4cx9QtcrErCM1xlfWCVblfMEY9yWvA2IVHpcjXEO0yY__86fSbv2Krw15Du2nlnMFyNH-BJVHVqhWsCxGPBzfQNJsAhD7BBCZyf37g_1UPWrJDOp0fH36gS9nRPLQZ1yN8sjsa9f79BIeWX-_LefcyFQi_By5tSRN2bIOlSqppjK_NPXuGpvSfjH0bjNcr5iWt4NYn6c1JWJMXp2hQxtH0pqYAAa8dYQsNlUSocjnMTNJWtRcafKyyVz96X4kt-m/file\n",
      "Reusing existing connection to uc0226b5ea0e46c5db64e70c39ff.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23616000 (23M) [application/octet-stream]\n",
      "Saving to: ‘./data/isbi.tar.gz’\n",
      "\n",
      "isbi.tar.gz         100%[===================>]  22,52M  3,36MB/s    in 11s     \n",
      "\n",
      "2019-06-14 21:47:59 (2,01 MB/s) - ‘./data/isbi.tar.gz’ saved [23616000/23616000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data:\n",
    "! wget -P ./data/ https://www.dropbox.com/s/0rvuae4mj6jn922/isbi.tar.gz\n",
    "! tar -xf ./data/isbi.tar.gz -C ./data/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final folder structure with training data:\n",
    "```bash\n",
    "catalyst-examples/\n",
    "    data/\n",
    "        isbi/\n",
    "            train-volume.tif\n",
    "            train-labels.tif\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tifffile\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/ea/197e476121949d8672c9d04b9a4321854747cd7c191012f83e3ebc3517c6/tifffile-2019.5.30-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 869kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /home/user/.virtualenvs/cp36/lib/python3.6/site-packages (from tifffile) (1.16.4)\n",
      "Installing collected packages: tifffile\n",
      "Successfully installed tifffile-2019.5.30\n"
     ]
    }
   ],
   "source": [
    "! pip install tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tiff\n",
    "\n",
    "images = tiff.imread('./data/isbi/train-volume.tif')\n",
    "masks = tiff.imread('./data/isbi/train-labels.tif')\n",
    "\n",
    "data = list(zip(images, masks))\n",
    "\n",
    "train_data = data[:-4]\n",
    "valid_data = data[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'UtilsFactory' has no attribute 'create_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-97eb8355124b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m train_loader = UtilsFactory.create_loader(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mopen_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'UtilsFactory' has no attribute 'create_loader'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from catalyst.data.augmentor import Augmentor\n",
    "from catalyst.dl.utils import UtilsFactory\n",
    "\n",
    "bs = 4\n",
    "num_workers = 4\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    Augmentor(\n",
    "        dict_key=\"features\",\n",
    "        augment_fn=lambda x: \\\n",
    "            torch.from_numpy(x.copy().astype(np.float32) / 255.).unsqueeze_(0)),\n",
    "    Augmentor(\n",
    "        dict_key=\"features\",\n",
    "        augment_fn=transforms.Normalize(\n",
    "            (0.5, ),\n",
    "            (0.5, ))),\n",
    "    Augmentor(\n",
    "        dict_key=\"targets\",\n",
    "        augment_fn=lambda x: \\\n",
    "            torch.from_numpy(x.copy().astype(np.float32) / 255.).unsqueeze_(0))\n",
    "])\n",
    "\n",
    "open_fn = lambda x: {\"features\": x[0], \"targets\": x[1]}\n",
    "\n",
    "loaders = collections.OrderedDict()\n",
    "\n",
    "train_loader = UtilsFactory.create_loader(\n",
    "    train_data, \n",
    "    open_fn=open_fn, \n",
    "    dict_transform=data_transform, \n",
    "    batch_size=bs, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=True)\n",
    "\n",
    "valid_loader = UtilsFactory.create_loader(\n",
    "    valid_data, \n",
    "    open_fn=open_fn, \n",
    "    dict_transform=data_transform, \n",
    "    batch_size=bs, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=False)\n",
    "\n",
    "loaders[\"train\"] = train_loader\n",
    "loaders[\"valid\"] = valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.models.segmentation import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from catalyst.dl.experiments import SupervisedRunner\n",
    "\n",
    "# experiment setup\n",
    "num_epochs = 10\n",
    "logdir = \"./logs/segmentation_notebook\"\n",
    "\n",
    "# model, criterion, optimizer\n",
    "model = Unet(num_classes=1, in_channels=1, num_channels=64, num_blocks=4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 40], gamma=0.3)\n",
    "\n",
    "\n",
    "# model runner\n",
    "runner = SupervisedRunner()\n",
    "\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from catalyst.dl.callbacks import InferCallback, CheckpointCallback\n",
    "loaders = collections.OrderedDict([(\"infer\", loaders[\"valid\"])])\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=loaders,\n",
    "    callbacks=[\n",
    "        CheckpointCallback(\n",
    "            resume=f\"{logdir}/checkpoints/best.pth\"),\n",
    "        InferCallback()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
    "\n",
    "for i, (input, output) in enumerate(zip(\n",
    "        valid_data, runner.callbacks[1].predictions[\"logits\"])):\n",
    "    image, mask = input\n",
    "    \n",
    "    threshold = 0.5\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, 'gray')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    output = sigmoid(output[0].copy())\n",
    "    output = (output > threshold).astype(np.uint8)\n",
    "    plt.imshow(output, 'gray')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, 'gray')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
